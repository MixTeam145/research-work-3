\documentclass[specialist,
substylefile = spbu_report.rtx,
subf,href,colorlinks=true, 12pt]{disser}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage[a4paper,
mag=1000, includefoot,
left=3cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm]{geometry}

\usepackage{graphicx,subcaption,ragged2e}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}


\newcommand{\traj}{\mathbf{X}}
\newcommand{\toeplitz}{\widetilde{\mathbf{C}}}
\newcommand{\transponse}{^\mathrm{T}}


\theoremstyle{definition}
\newtheorem{definition}{Определение}
\newtheorem{algorithm}{Алгоритм}

\newcommand{\R}{\mathbb{R}}

\newcommand{\bfxi}{\boldsymbol{\xi}}

\include{letters_series_mathbb.tex}

\begin{document}
	%
	% Титульный лист на русском языке
	%
	% Название организации
	\institution{%
		Санкт-Петербургский государственный университет\\
		Прикладная математика и информатика
	}
	
	\title{Отчет по учебной практике 3 (научно-исследовательской работе) (семестр 6)}
	
	% Тема
	\topic{Метод Монте-Карло SSA для многомерных временных рядов}
	
	% Автор
	\author{Потешкин Егор Павлович}
	\group{группа 20.Б04-мм}
	
	% Научный руководитель
	\sa       {Голяндина Нина Эдуардовна \\%
		Кафедра Статистического Моделирования}
	\sastatus {к.\,ф.-м.\,н., доцент}
	
	% Город и год
	\city{Санкт-Петербург}
	\date{\number\year}
	
	\maketitle
	\tableofcontents
	\intro
	Метод Singular Spectrum Analysis (SSA) является мощным инструментом для анализа временных рядов. Он позволяет разложить ряд на интерпретируемые компоненты, такие как тренд, периодические колебания и шум, что значительно упрощает процесс анализа. Метод Monte-Carlo SSA, в свою очередь, решает задачу обнаружения сигнала в шуме~\cite{Golyandina_2023}.
	
	Однако, вариант Monte-Carlo SSA для анализа многомерных временных рядов мало исследован. В работе~\cite{Larin_2022} рассматривается применение метода Monte Carlo SSA для анализа многомерных временных рядов, и авторы сталкиваются с проблемой отсутствия реализации Тёплицева варианта MSSA в пакете Rssa~\cite{Rssa}.
	
	В этой работе была поставлена задача реализовать двумя способами метод Toeplitz MSSA, сравнить их между собой и с обычным MSSA как для методов оценки сигнала, так и для использования в Monte-Carlo MSSA.
	
	%Рассмотрим вещественнозначный временной ряд, т.е. последовательность вещественнозначных чисел, упорядоченных по времени. Метод Singular Spectrum Analysis (SSA) позволяет представить такой ряд в виде суммы интерпретируемых компонент, таких как тренд, периодические компоненты и шум, что упрощает процесс анализа временного ряда.
	
	%В повседневной жизни чаще всего временные ряды встречаются в виде детерминированного сигнала и случайного шума. Тогда возникает задача обнаружения сигнала в шуме. Метод Monte-Carlo SSA "--- решение этой задачи.  
	
	%Существует вариант Monte-Carlo SSA для анализа многомерных временных рядов, однако этот вариант мало исследован. В работе~\cite{Larin_2022} были численно исследованы свойства метода Monte-Carlo SSA, однако исследование не было завершено полностью, возникла проблема необходимости реализации Тёплицева варианта Multivariate SSA (MSSA), но ее не было в используемом пакете Rssa~\cite{Rssa}.
	
	%Целью данной работы является реализация метода Тёплицева MSSA двумя способами, а также сравнение способов между собой и с базовой версией MSSA. Было произведено сравнение точности методов в восстановлении сигнала и в использовании в Monte-Carlo MSSA.  

	\chapter{Метод MSSA}
	Метод Multivariate Singular Spectrum Analysis (сокращенно MSSA) состоит из четырех этапов: \emph{вложения}, \emph{разложения}, \emph{группировки} и \emph{диагонального усреднения}.
	Давайте начнем с общих частей всех версий алгоритмов SSA. Этими общими частями являются процедура вложения и диагонального усреднения (ганкелизации).
	\begin{definition}
		Пусть $\tX$ "--- одномерный временной ряд длины $N$. Выберем параметр $L$, называемый \emph{длиной окна}, $1<L<N$. Рассмотрим $K=N-L+1$ векторов вложения $X_i=(x_{i},\ldots, x_{i+L-1})^\rmT,\ 1\leqslant j \leqslant K$. Определим оператор вложения $\cT$ следующим образом:
		\begin{equation}\label{eq:embedding}
			\cT(\tX)=\bfX=[X_1:\ldots:X_K]=
			\begin{pmatrix}
				x_1 & x_2 & \cdots & x_K \\
				x_2 & x_3 & \cdots & x_{K+1} \\
				\vdots & \vdots & \ddots & \vdots \\
				x_L & x_{L+1} & \cdots & x_N 
			\end{pmatrix}.
		\end{equation}
	\end{definition}
	\begin{definition}
		Матрицу $\bfX$ из~\eqref{eq:embedding} называют траекторной матрицей.
	\end{definition}\noindent
	Заметим, что матрица $\bfX$ является \emph{ганкелевой}, т.е на всех ее побочных диагоналях стоят одинаковые элементы, а оператор $\cT$ задает взаимно-однозначное соответствие между множеством временных рядов длины $N$ и множеством ганкелевых матриц $L\times K$.
		\begin{definition}
		Пусть $\bfY=\{y_{ij}\}_{i,j=1}^{L,K}$ "--- некоторая матрица. Определим оператор ганкелизации $\cH$:
		\begin{equation}\label{eq:averaging}
			(\cH(\bfY))_{ij}=\sum_{(l,k)\in A_s}y_{lk}/w_s,
		\end{equation}
		где $s=i+j-1$, $A_s=\{(l,k)\, :\, l+k=s+1,\, 1\leqslant l\leqslant L,\, 1\leqslant k\leqslant K\}$ и $w_s=|A_s|$ "--- количество элементов в множестве $A_s$. Это соответствует
		усреднению элементов матрицы $\bfY$ по побочным диагоналям.
	\end{definition}
	\section{Описание метода}
	Рассмотрим вещественнозначные одномерные временные ряды $\tX^{(d)}=(x_1^{(d)}, x_2^{(d)},\ldots, x_{N_d}^{(d)})$ длины $N_d>2$, $d=1,\ldots,D$. Составим из этих рядов $\tX=\{\tX^{(d)}\}_{d=1}^D$ "--- $D$-канальный временной ряд с длинами $N_d$.
	%\begin{definition}
	%	\textbf{\emph{Оператор вложения}} $\cT$ "--- линейное отображение, которое преобразует %временной ряд $\tX$ в траекторную матрицу $\bfX$.
	%\end{definition}
	\subsection{Вложение}\label{sect:embedding}
	Зафиксируем $L$, $1<L<\min(N_1,\ldots,N_D)$. Для каждого ряда $\tX^{(d)}$ составим траекторную матрицу $\bfX^{(d)}$ . Обозначим $K=\sum_{d=1}^D K_d$. Результатом этапа вложения является траекторная матрица многоканального временного ряда
	\begin{equation}\label{eq:traj}
		\bfX=[\cT(\tX^{(1)}):\ldots:\cT(\tX^{(D)})]=[\traj^{(1)}:\ldots:\traj^{(D)}].
	\end{equation}
	\subsection{Разложение}
	Задача этапа разложения "--- разбить траекторную матрицу $\traj$ в сумму матриц ранга 1. В базовой версии MSSA используется сингулярное разложение (SVD).
	
	Положим $\mathbf{S}=\traj\traj\transponse$. Пусть $\lambda_i$ "--- собственные числа, а $U_i$ "--- ортонормированная система векторов матрицы $\mathbf{S}$. Упорядочим $\lambda_i$ по убыванию и найдем $p$ такое, что $\lambda_p>0$, а $\lambda_{p+1}=0$. Тогда
	\[
	\traj=\sum_{i=1}^p\sqrt{\lambda_i}U_iV_i\transponse=\sum_{i=1}^p\traj_i,
	\] 
	где $V_i=\traj\transponse U_i/\sqrt{\lambda_i}$. Тройку $(\sqrt{\lambda_i}, U_i, V_i)$ принято называть $i$-й собственной тройкой сингулярного разложения, $\sqrt{\lambda_i}$ "--- сингулярным числом, $U_i$ "--- левым сингулярным вектором, а $V_i$ "--- правым сингулярным вектором. Отметим, что левые сингулярные векторы имеют размерность $L$, а правые сингулярные вектора "--- размерность $K$. 
	\subsection{Группировка}\label{sect:grouping}
	На этом шаге множество индексов $I=\{1,\ldots,p\}$ разбивается на $m$ непересекающихся множеств $I_m,\ldots,I_m$ и матрица $\traj$ представляется в виде суммы
	\[
	\traj = \sum_{k=1}^m \traj_{I_k},
	\]
	где $\traj_{I_k}=\sum_{i\in I_k}\traj_i$.
	\subsection{Диагональное усреднение}\label{sect:averaging}
	Финальным шагом MSSA является преобразование каждой матрицы $\traj_{I_k}$, составленной в разделе~\ref{sect:grouping}, в $D$-канальный временной ряд следующим образом:
	\begin{equation}
		\widetilde\tX_{I_k}=\left\{\cT^{-1}\circ\cH\left(\bfX_{I_k}^{(d)}\right)\right\}_{d=1}^D,
	\end{equation}
	где $\cT$ "--- оператор вложения~\eqref{eq:embedding}, $\cH$ "--- оператор ганкелизации~\eqref{eq:averaging}.
	
	%Пусть $\mathbf Y=(y_{ij})$ "--- матрица размера $L\times K$. Положим $L^*=\min(L,K)$, %$K^*=\max(L,K)$ и $N=L+K-1$. Пусть $y^*_{ij}=y_{ij}$, если $L<K$, и $y^*_{ij}=y_{ji}$ иначе. %\textit{Диагональное усреднение} переводит матрицу $\mathbf{Y}$ в ряд $g_1,\ldots,g_N$ по %формуле
	%\[
	%g_k=
	%\begin{cases}
	%	{\displaystyle\frac{1}{k}\sum_{m=1}^{k} y^*_{m,k-m+1}},&\text{при }1\leqslant k<L^*\\
	%	{\displaystyle\frac{1}{L^*}\sum_{m=1}^{L^*} y^*_{m,k-m+1}},&\text{при }L^*\leqslant %k\leqslant K^* \\
	%	{\displaystyle\frac{1}{N-k+1}\sum_{m=k-K^*+1}^{N-K^*+1}y^*_{m,k-m+1}},&\text{при }K^*< %k\leqslant N
	%\end{cases}
	%\]
	
	%Из~\eqref{eq:traj} следует, что $\traj_{I_k}$ можно представить в следующем виде:
	%\[
	%\traj_{I_k}=[\traj^{(1)}_{I_k}:\ldots:\traj^{(D)}_{I_k}].
	%\]
	%Тогда, чтобы получить $D$-канальный временной ряд, применим диагональное усреднение к каждой %матрице $\traj_{I_k}^{(d)}$, $d=1,\ldots,D$.
	\subsection{Частный случай}
	При $D=1$ $\tX$ "--- одномерный временной ряд, и приведенный выше алгоритм совпадает с алгоритмом Basic SSA, описанный в~\cite{ssa_an}.
	
	\section{Модификации метода}
	\subsection{Тёплицев MSSA}\label{toeplitz}
	В случае анализа стационарных рядов можно улучшить базовый метод, используя другое разложение матрицы $\bfX$. Для начала введем следующее понятие.
	
	\begin{definition}
	Пусть $\tX=(x_1,\ldots,x_N)$ "--- одномерный временной ряд и $L$ "--- фиксированное. Тёплицевой $L$-ковариационной матрицей называют матрицу $\widetilde\bfC$ с элементами
	\[
	\widetilde{c}_{ij}=\frac{1}{N-|i-j|}\sum_{n=1}^{N-|i-j|} x_nx_{n+|i-j|},\quad 1\leqslant i,j \leqslant L.
	\]
	\end{definition}
	
	Пусть теперь $\tX=\{\tX^{(d)}\}_{d=1}^D$ "--- $D$-канальный временной ряд с длинами $N_d$. Тогда можно получить разложение $\bfX$ двумя способами:
	\begin{enumerate}
		\item Пусть $\widetilde\bfC_1,\ldots,\widetilde\bfC_D$ "--- тёплицевы матрицы для каждого канала. Рассмотрим $\widetilde\bfC=\sum_{d=1}^D \widetilde\bfC_d$. Найдем ортонормированные собственные векторы $H_1,\ldots,H_L$ матрицы $\widetilde\bfC$ и разложим траекторную матрицу $\traj$ следующим образом:
		\begin{equation}\label{eq:sum_decomposition}
			\mathbf{X}=\sum_{i=1}^L \sigma_i H_i Q_i^\mathrm{T},
		\end{equation}
		где $Z_i=\mathbf{X^T}U_i$, $Q_i=Z_i/\|Z_i\|$ и $\sigma_i=\|Z_i\|$.
		\item Если все каналы одинаковой длины $N$, можно рассмотреть блочную матрицу размера $DK\times DK$:
		\[
		\bfT=\begin{pmatrix}
			\bfT_{1,1} & \bfT_{1,2} & \cdots & \bfT_{1,D} \\
			\bfT_{2,1} & \bfT_{2,2} & \cdots & \bfT_{2,D} \\
			\vdots  & \vdots  & \ddots & \vdots  \\
			\bfT_{D,1} & \bfT_{D,D} & \cdots & \bfT_{D,D}
		\end{pmatrix},
		\]
		где $K=N-L+1$.
		Элементы каждого блока $\bfT_{lk}$ имеют вид
		\[
		t^{(lk)}_{ij}=\frac{1}{\tilde N}\sum_{n=\max(1,1+i-j)}^{\min(N,N+i-j)} x^{(l)}_nx^{(k)}_{n+j-i},\quad 1\leqslant i,j\leqslant K,
		\]
		где $\tilde N=\min(N,N+i-j)-\max(1,1+i-j)+1$. Найдя ортонормированные собственные векторы $Q_1,\ldots,Q_{DK}$ матрицы $\bfT$, получаем разложение
		\begin{equation}\label{eq:block_decomposition}
			\mathbf{X}=\sum_{i=1}^{DK} \sigma_i H_i Q_i^\mathrm{T},    
		\end{equation}
		где $Z_i=\mathbf{X}Q_i$, $H_i=Z_i/\|Z_i\|$ и $\sigma_i=\|Z_i\|$.
	\end{enumerate}
	Шаги группировки и диагонального усреднения можно оставить в том виде, в котором они представлены в разделе~\ref{sect:grouping} и в разделе~\ref{sect:averaging}.
	
	Для конкретности, будем называть первый метод Sum, а второй "--- Block. Стоит отметить, что в Sum собственные векторы матрицы $\toeplitz$ "--- аналоги левых сингулярных векторов матрицы $\mathbf{X}$, в то время как в Block собственные векторы матрицы $\mathbf{T}$ "--- аналоги правых сингулярных векторов. В дальнейшем будем просто называть их \emph{левыми} и \emph{правыми векторами}.
	\section{Выбор длины окна (численное исследование)}
	Посмотрим на точность базового и модифицированных методов, для разных значений параметра $L$, наподобие работы~\cite{Golyandina_2015}. Рассмотрим следующий двухканальный временной ряд: $(\tF^{(1)}, \tF^{(2)})=(\tH^{(1)},\tH^{(2)}) + (\tN^{(1)},\tN^{(2)})$, где $\tH^{(1)}$, $\tH^{(2)}$ "--- гармоники, а $\tN^{(1)}$, $\tN^{(2)}$ "--- независимые реализации гауссовского белого шума. Гауссовский белый шум "--- стационарный случайный процесс, имеющий нормальное распределение. Как и в~\cite{Golyandina_2015}, пусть $N=71$, дисперсия шумовых компонент $\sigma^2=25$, число повторений равно 10000. Рассмотрим 2 случая:
	
	\begin{enumerate}
		\item Одинаковые периоды:
		\[
		h_n^{(1)}=30\cos(2\pi n/12),\quad h_n^{(2)}=20\cos(2\pi n/12),\quad n=1,\ldots N.
		\]
		\item Разные периоды:
		\[
		h_n^{(1)}=30\cos(2\pi n/12),\quad h_n^{(2)}=20\cos(2\pi n/8),\quad n=1,\ldots N.
		\]
	\end{enumerate}
		\begin{table}[h]
		\centering
		\caption{MSE восстановления сигнала.}
		\begin{tabular}{cccccc}\hline
			Случай 1 & $L=12$ & $L=24$ & $L=36$ & $L=48$ & $L=60$\\
			\hline
			MSSA & $3.18$ & $1.83$ & $1.59$ & $\mathbf{1.47}$ & $2.00$\\
			\hline
			SSA & $3.25$ & $\mathbf{2.01}$ & $\mathbf{2.00}$ & $\mathbf{2.01}$ & $3.25$\\
			\hline
			Sum &  $3.17$ & $1.75$ & $1.44$ & $\mathbf{1.32}$ & $\mathbf{1.33}$\\
			\hline
			Block & $1.39$ & $\mathbf{1.26}$ & $\mathbf{1.25}$ & $1.33$ & $1.97$\\
			\hline
		\end{tabular}
		\begin{tabular}{cccccc}\hline
			Случай 2 & $L=12$ & $L=24$ & $L=36$ & $L=48$ & $L=60$\\
			\hline
			MSSA & $6.91$ & $3.77$ & $3.07$ & $\mathbf{2.88}$ & $3.84$\\
			\hline
			SSA & $3.23$ & $\mathbf{2.01}$ & $\mathbf{2.00}$ & $\mathbf{2.01}$ & $3.23$\\
			\hline
			Sum & $6.88$ & $3.65$ & $2.64$ & $2.37$ & $\mathbf{2.27}$\\
			\hline
			Block & $4.47$ & $3.67$ & $\mathbf{3.22}$ & $\mathbf{3.23}$ & $3.8$\\
			\hline
		\end{tabular}
		\label{tab:mse}
	\end{table}
	В таблице~\ref{tab:mse} представлены результаты восстановления сигнала для разных $L$. Данные для методов SSA и MSSA были взяты из работы~\cite{Golyandina_2015}. Наиболее точные результаты для каждого метода были выделены жирным шрифтом. Как видим из таблицы~\ref{tab:mse}, в обоих случаях метод Sum показывал наилучший результат для $L>(N+1)/2$, в то время как метод Block наиболее точен при длине окна, близкой к половине длины ряда, причем оба метода в случае одинаковых периодов показывают лучше результат, чем MSSA.

	\chapter{Метод Monte-Carlo MSSA}
	\section{Постановка задачи}
	Рассмотрим задачу поиска сигнала (не случайной составляющей) в многоканальном временном ряде. Нулевая гипотеза $H_0$ "--- отсутствие сигнала (ряд состоит из чистого шума). Тогда альтернатива $H_1$ "--- ряд содержит сигнал, например, периодическую составляющую.
	\begin{definition}
		Случайный вектор $\boldsymbol{\xi}=(\xi_1,\dots,\xi_N)$ называют красным шумом с параметрами $\varphi$ и $\delta$, если $\xi_n = \varphi\xi_{n-1} + \delta\varepsilon_n$, где $0<\varphi<1$, $\varepsilon_n$ — белый гауссовский шум со средним значением 0 и дисперсией 1 и $\xi_1$ имеет нормальное распределение с нулевым средним и дисперсией $\delta^2/(1-\varphi^2)$.
	\end{definition}
	%\begin{definition}
	%	Пусть $\bfxi$ "--- красный шум с параметрами $\varphi$ и $\delta$. Теоретическая %корреляционная матрица красного шума $\bfxi$ "--- матрица с элементами $\varphi^{|i-j|}$.
	%\end{definition}
	В данной главе под шумом будем подразумевать именно красный, причем с известными параметрами.  Также будем рассматривать только односторонние критерии.
	\section{Одиночный тест}
	Пусть $\bfxi=\{\bfxi^{(d)}\}_{d=1}^D$ "--- $D$-канальный красный шум. Зафиксируем длину окна $L$ и обозначим траекторную матрицу ряда $\boldsymbol{\xi}$ как $\mathbf\Theta$. Рассмотрим вектор $W\in \R^{L}$ такой, что $\|W\|=1$. Введем величину
	\[
	p=\|\mathbf{\Theta}\transponse W_k\|^2.
	\]
	Статистикой критерия является величина
	\[
	\widehat{p}=\|\traj\transponse W\|^2.
	\]
	Если вектор $W$ "--- синусоида с частотой $\omega$, то $\widehat{p}$ отражает вклад частоты $w$ в исходный ряд.
	
	Рассмотрим алгоритм статистического критерия проверки наличия сигнала в ряде с проекцией на один вектор $W$, описанный в работе~\cite{Golyandina_2023}.
	\begin{algorithm}{Одиночный тест~\cite{Golyandina_2023}}
		\begin{enumerate}
			\item Построить статистику критерия $\widehat p$.
			\item Построить доверительную область случайной величины $p$: интервал от нуля до $(1-\alpha)$-квантиля, где $\alpha$ "--- уровень значимости.
			\item Если $\widehat p$	не попадает в построенный интервал "--- $H_0$ отвергается. 
		\end{enumerate}
	\end{algorithm}
	Построенная доверительная область называется \textit{прогнозируемым интервалом} с уровнем доверия $1-\alpha$.
	
	В большинстве случаев, распределение $p$ неизвестно. Поэтому оно оценивается методом Монте-Карло: берется $G$ реализаций случайной величины $\boldsymbol\xi$, для каждой вычисляется $p$ и строится эмпирическое распределение. В связи с этим описанный выше алгоритм называют методом Monte-Carlo SSA. 
	\section{Множественный тест}
	Пусть теперь частоты периодических компонент неизвестны (что не редкость на практике), но известен диапазон частот и нужно проверить, что в ряде присутствует сигнал с хотя бы одной частотой из заданного диапазона. Тогда нулевая гипотеза $H_0$ о том, что ряд не содержит сигнала ни на одной из частот из рассматриваемого диапазона, а альтернатива $H_1$ "--- ряд содержит сигнал с хотя бы одной частотой, принадлежащей рассматриваемому диапазону.
	
	Пусть $W_1,\ldots,W_H$ "--- вектора для проекции. В таком случае нужно построить $H$ предсказательных интервалов по выборкам $P_k=\{p_{ki}\}_{i=1}^G$ с элементами
	\begin{equation}
		p_{ki}=\|\mathbf{\Xi}_i\transponse W_k\|^2,\quad i=1,\ldots,G;\ k=1,\ldots,H,
	\end{equation}
	где $G$ "--- количество суррогатных реализаций $\boldsymbol{\xi}$, $\mathbf{\Xi}_i$ "--- траекторная матрица $i$-й реализации $\boldsymbol{\xi}$. 
	
	В работе~\cite{Golyandina_2023} подробна описана проблема множественного тестирования, когда вероятность ложного обнаружения периодической составляющей для одной из рассматриваемых частот (групповая ошибка I рода) неизвестна и значительно превышает заданный уровень значимости (частота ошибок одиночного теста), и ее решение. Приведем модифицированный алгоритм построения критерия в случае множественного тестирования, который будем использовать в дальнейшем.
	\begin{algorithm}{Multiple MC-SSA~\cite{Golyandina_2023}}
	\begin{enumerate}
		\item Для $k=1,\dots,H$ вычисляется статистика $\widehat{p}_k$, выборка $P_k=\{p_{ki}\}_{i=1}^G$, ее среднее $\mu_k$ и стандартное отклонение $\sigma_k$.
		\item Вычисляется $\mathbf{\eta}=(\eta_1,\dots,\eta_G)$, где
		\[
		\eta_i=\max_{1\leqslant k\leqslant H}(p_{ki}-\mu_k)/\sigma_k,\quad i=1,\dots,G.
		\]
		\item Находится $q_k$ как выборочный $(1-\alpha)$-квантиль $\eta$, где $\alpha$ "--- уровень значимости.
		\item Нулевая гипотеза не отвергается, если
		\[
		\max_{1\leqslant k\leqslant H}(\widehat{p}_k-\mu_k)/\sigma_k<q.
		\]
		\item Если $H_0$ отвергнута, вклад $W_k$ (и соответствующей частоты) существеннен, если $\widehat{p}_k$ превосходит $\mu_k+q\sigma_k$. Таким образом, $[0,\mu_k+q\sigma_k]$ считаются скорректированными интервалами прогнозирования.
	\end{enumerate}
	\end{algorithm}
	\section{Выбор векторов для проекции}
	Для начала отметим, что в одномерном случае можно рассматривать как проекции на собственные вектора, так и на факторные  "--- не имеет значения, поскольку собственные векторы становятся факторными при изменении длины окна с $L$ на $N-L+1$. А в многомерном случае это не так по построению матрицы~\eqref{eq:traj}, поэтому их нужно рассматривать по-отдельности.
	
	Перечислим основные способы выбора векторов для проекции. Первый вариант "--- рассматривать собственные векторы теоретической ковариационной матрицы красного шума. Пусть $\bfxi=\{\bfxi^{(d)}\}_{d=1}^D$ "--- $D$-канальный красный шум с параметрами $\varphi_d$ и $\delta_d$.  При рассмотрении собственных векторов матрица, разложение которой дает эти собственные векторы, имеет вид $\sum_{d=1}^D\sigma_d\{\varphi_d^{|i-j|}\}_{i,j=1}^{L}$, а при рассмотрении факторных векторов матрица имеет блочно-диагональный вид с блоками $\sigma_d\{\varphi_d^{|i-j|}\}_{i,j=1}^{K_d}$, где $\sigma_d=\delta_d^2/(1-\varphi_d^2)$. Такой вариант в обоих случаях дает точный критерий при любой длине окна. 
	
	Второй вариант "--- рассматривать левые или правые векторы матрицы $\bfX$. Этот вариант, вообще говоря, радикальный, поскольку, если рассматривать реализацию шума $\bfxi$, левые и правые векторы его траекторной матрицы всегда будут разными (будут подстраиваться под конкретную реализацию), в то время как в первом варианте векторы зависят только от его параметров. Однако, используя поправку, описанную в работе~\cite{Larin_2022}, можно сделать радикальный критерий точным.
	
  
	Если рассматривать левые векторы, то их длина равна $L$, а сами векторы отображают общую структуру для всех рядов. Если же рассматривать правые, то их длина равна $D(N-L+1)$, а сами векторы имеют составную структуру, где каждому ряду соответствует вектор длины $N-L+1$.
	
	\begin{definition}
		ROC-кривая "--- это кривая, задаваемая параметрически
		\[
		\begin{cases}
		x=\alpha_I(\alpha)\\
		y=\beta(\alpha)
		\end{cases},\quad \alpha\in[0,1],
		\]
		где $\alpha_I(\alpha)$ "--- функция зависимости ошибки первого рода $\alpha_I$ от уровня значимости $\alpha$, $\beta(\alpha)$ "--- функция зависимости мощности $\beta$ от уровня значимости $\alpha$.
	\end{definition}
	С помощью ROC-кривых можно сравнивать по мощности неточные (в частности, радикальные) критерии. Отметим, что для точного критерия ROC-кривая совпадает с графиком мощности, так как $\alpha_I(\alpha)=\alpha$.
	\section{Численное сравнение методов}
	В одномерном случае было установлено, что если вместо SVD разложения матрицы $\traj$ использовать тёплицево, то радикальность критерия уменьшается. Установим, что будет в многомерном случае, если использовать модификации, описанные в разделе~\ref{toeplitz}.
	
	Пусть количество каналов равно двум, количество суррогатных реализаций красного шума $G=1000$. Для оценки ошибки первого рода, будем рассматривать красный шум $\bfxi$ с параметрами $\varphi=0.7$ и $\delta=1$, а для оценки мощности будет рассматривать временной ряд $\tX=\tS+\bfxi$, где $\tS$ "--- сигнал с элементами
	\[
	s_n^{(1)}=s_n^{(2)}=\cos(2\pi\omega n),\quad n=1,\ldots, N,
	\]
	где $\omega=0.075$, $N=100$.
	
	Построим графики ошибки первого рода и ROC-кривые для каждой длины окна $L=10$, $20$, $50$, $80$, $90$. Будем воспринимать ROC-кривую как график мощности критерия, к которому была применена поправка, описанная в работе~\cite{Larin_2022}. 
	
	На рис.~\ref{fig:sum_ev} и~\ref{fig:sum_fa} векторы для проекции были взяты из разложения~\eqref{eq:sum_decomposition}. На рис.~\ref{fig:sum_ev_a} видно, что при $L>20$ метод радикальный, а наибольшая мощность достигается при $L=90$. На рис.~\ref{fig:sum_fa_a} отчетливо заметно, что метод радикальный для всех $L$. Наибольшая мощность наблюдается при $L=90$, но отметим, что из-за слишком большой ошибки первого рода построить ROC-кривую на промежутке [0,3) для $L=50$ и на всем промежутке для $L=10$ и $L=20$ не получилось.
	
	На рис.~\ref{fig:block_ev} и~\ref{fig:block_fa} векторы для проекции были взяты из разложения~\eqref{eq:block_decomposition}. Если рассматривать проекцию на левые векторы, то на рис.~\ref{fig:block_ev_a} видно, что метод радикальный, а наибольшая мощность достигается при $L=20$. Проекция на правые векторы также дает радикальный критерий, как видно на рис.~\ref{fig:block_fa_a}. Наибольшая мощность наблюдается при $L=80$, но из-за слишком большой ошибки первого рода ROC-кривую для $L=10$ и $L=20$, для которых метод, предположительно, имеет б\'oльшую мощность, удалось построить не на всем промежутке.
	\begin{figure}
		\captionsetup[subfigure]{justification=Centering}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{type1error_sum_ev.pdf}
			\caption{Ошибка первого рода (Sum).}
			\label{fig:sum_ev_a}
		\end{subfigure}\hspace{\fill}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{type1error_mssa_ev.pdf}
			\caption{Ошибка первого рода (базовый MSSA).}
		\end{subfigure}
		\bigskip
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{roc_sum_ev.pdf}
			\caption{ROC-кривая (Sum).}
		\end{subfigure}\hspace{\fill}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{roc_mssa_ev.pdf}
			\caption{ROC-кривая (базовый MSSA).}
		\end{subfigure}
		\caption{Сравнение методов Sum и базового MSSA (проекция на левые векторы).}
		\label{fig:sum_ev}
	\end{figure}
	\begin{figure}
		\captionsetup[subfigure]{justification=Centering}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{type1error_sum_fa.pdf}
			\caption{Ошибка первого рода (Sum).}
			\label{fig:sum_fa_a}
		\end{subfigure}\hspace{\fill}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{type1error_mssa_fa.pdf}
			\caption{Ошибка первого рода (базовый MSSA).}
		\end{subfigure}
		\bigskip
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{roc_sum_fa.pdf}
			\caption{ROC-кривая (Sum).}
		\end{subfigure}\hspace{\fill}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{roc_mssa_fa.pdf}
			\caption{ROC-кривая (базовый MSSA).}
		\end{subfigure}
		\caption{Сравнение методов Sum и базового MSSA (проекция на правые векторы).}
		\label{fig:sum_fa}
	\end{figure}
		\begin{figure}
		\captionsetup[subfigure]{justification=Centering}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{type1error_block_ev.pdf}
			\caption{Ошибка первого рода (Block).}
			\label{fig:block_ev_a}
		\end{subfigure}\hspace{\fill}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{type1error_mssa_ev.pdf}
			\caption{Ошибка первого рода (базовый MSSA).}
		\end{subfigure}
		\bigskip
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{roc_block_ev.pdf}
			\caption{ROC-кривая (Block).}
		\end{subfigure}\hspace{\fill}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{roc_mssa_ev.pdf}
			\caption{ROC-кривая (базовый MSSA).}
		\end{subfigure}
		\caption{Сравнение методов Block и базового MSSA (проекция на левые векторы).}
		\label{fig:block_ev}
	\end{figure}
		\begin{figure}
		\captionsetup[subfigure]{justification=Centering}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{type1error_block_fa.pdf}
			\caption{Ошибка первого рода (Block).}
			\label{fig:block_fa_a}
		\end{subfigure}\hspace{\fill}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{type1error_mssa_fa.pdf}
			\caption{Ошибка первого рода (базовый MSSA).}
		\end{subfigure}
		\bigskip
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{roc_block_fa.pdf}
			\caption{ROC-кривая (Block).}
		\end{subfigure}\hspace{\fill}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=0.75\textwidth]{roc_mssa_fa.pdf}
			\caption{ROC-кривая (Block).}
		\end{subfigure}
		\caption{Сравнение методов Block и базового MSSA (проекция на правые векторы).}
		\label{fig:block_fa}
	\end{figure}
	\section{Выводы}
	Подведем итоги. На данный момент для метода Sum оптимальной длиной окна является $L=90$, если рассматривать проекцию как на левые, так и на правые векторы. Для метода Block оптимальной длиной окна является $L=20$, если рассматривать проекцию на левые векторы, и $L=80$, если рассматривать проекцию на правые векторы.
	
	Также все методы, кроме Sum, с проекцией на левые вектора сильно радикальные. Поэтому рекомендуется использовать именно этот вариант модификации. 
	\conclusion
	В ходе данной работы для реализации двух методов Тёплицева MSSA был использован язык программирования $\tR$. Было получено, что в точности восстановления сигнала оба метода в большинстве случаев показывают лучший результат, чем обычный MSSA. Но в Monte-Carlo SSA метод Sum более предпочтителен, чем метод Block, что важно ввиду его простоты в реализации и структуры, подходящей под пакет Rssa~\cite{Rssa}.
	
	В дальнейшем предполагается использовать метод Sum с проекцией на левые векторы, оптимизировать его реализацию и продолжить исследование метода Monte-Carlo MSSA.
	
	\bibliographystyle{ugost2008}
	\bibliography{report}
\end{document}